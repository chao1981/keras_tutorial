from keras.datasets import cifar10
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from keras.callbacks import LearningRateScheduler, TensorBoard,ModelCheckpoint

######################hyper-parameter##################################
batch_size = 32
nb_epoch = 10
data_augmentation =True #是否数据扩充，主要针对样本过小方案



#########################data_preprocessing############################
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
print('X_train shape:', X_train.shape)
print(X_train.shape[2], 'train samples')
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
# data preprocessing
X_train[:,:,:,0] = X_train[:,:,:,0]-123.680
X_train[:,:,:,1] = X_train[:,:,:,1]-116.779
X_train[:,:,:,2] = X_train[:,:,:,2]-103.939
X_test[:,:,:,0] = X_test[:,:,:,0]-123.680
X_test[:,:,:,1] = X_test[:,:,:,1]-116.779
X_test[:,:,:,2] = X_test[:,:,:,2]-103.939

X_train /= 255
X_test /= 255
nb_classes = 10

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)


if data_augmentation:
    print('Using real-time data augmentation.')
    # this will do preprocessing and realtime data augmentation
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images
    # compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied)
    datagen.fit(X_train)

######################model############################################
model = Sequential()
model.add(Conv2D(32, (3, 3),padding='same',input_shape=X_train.shape[1:]))
model.add(Activation('relu'))#激活函数
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

#################optimization_lossfunction##############################
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

##################model loading########################################
#model.save_weights(weights,accuracy=False)

##################model saving########################################
checkpoint = ModelCheckpoint('best_model_improved.h5',  # model filename
                             monitor='val_loss', # quantity to monitor
                             verbose=0, # verbosity - 0 or 1
                             save_best_only= True, # The latest best model will not be overwritten
                             mode='auto') # The decision to overwrite model is m

def scheduler(epoch):
  if epoch < 80:
    return 0.01
  if epoch < 160:
    return 0.01
  return 0.001

tb_cb = TensorBoard(log_dir='./log', histogram_freq=0)
change_lr = LearningRateScheduler(scheduler)
cbks = [checkpoint,change_lr,tb_cb]
#########################training#########################################
if not data_augmentation:
    print('Not using data augmentation.')
    result=model.fit(X_train, Y_train,
              batch_size=batch_size,
              epochs=nb_epoch,
              callbacks=cbks,
              validation_data=(X_test, Y_test),
              shuffle=True)
else:
    print('Using real-time data augmentation.')
    # fit the model on the batches generated by datagen.flow()
    result=model.fit_generator(datagen.flow(X_train, Y_train,batch_size=batch_size),
                        samples_per_epoch=X_train.shape[0],
                        epochs=nb_epoch,
                        callbacks=cbks,
                        validation_data=(X_test, Y_test))

##################model saving########################################
#model.save_weights(weights,accuracy=False)



##################model predict#################################################
scores = model.evaluate(X_test, Y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))


##################Visualization#################################################
plt.figure
plt.plot(result.epoch,result.history['acc'],label="acc")
plt.plot(result.epoch,result.history['val_acc'],label="val_acc")
plt.scatter(result.epoch,result.history['acc'],marker='*')
plt.scatter(result.epoch,result.history['val_acc'])
plt.legend(loc='under right')
plt.show()
plt.figure
plt.plot(result.epoch,result.history['loss'],label="loss")
plt.plot(result.epoch,result.history['val_loss'],label="val_loss")
plt.scatter(result.epoch,result.history['loss'],marker='*')
plt.scatter(result.epoch,result.history['val_loss'],marker='*')
plt.legend(loc='upper right')
plt.show()
